1. Отличия между версиями MobileNet
MobileNet_v1 - MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications [https://arxiv.org/pdf/1704.04861.pdf]
Сеть использует deapthwise separable convolutions вместо стандартной свертки. При стандартной свертке у фильтров столько же каналов, сколько у обрабатываемого тензора. При deapthwise separable convolutions каждый канал тензора фильтруется своим одноканальным фильтром (depthwise convolution), а затем получившиеся признаки комбинируются с помощью многоканальных 1х1 фильтров, создвавая новые признаки (pointwise convolution). Это позволяет значительно уменьшить кол-во операций и параметров, а также увеличить быстродействие. При этом точность падает несущественно. Также в этой модели используются два гиперпараметра для контроля соотношения между быстродействием и точностью - width multiplier (равномерно уменьшает/увеличивает количество каналов в слоях) и resolution multiplier (контролирует разрешение обрабатываемых изображений).

Mobilenet_v2 - MobileNetV2: Inverted Residuals and Linear Bottlenecks [https://arxiv.org/pdf/1801.04381.pdf]
Сеть основана на модуле inverted residual with linear bottleneck. Этот модуль принимает сжатый тензор с маленьким количеством каналов, расширяет его с помощью 1х1-свертки (увеличивая количество каналов), фильтрует с помощью одноканальных фильтров (depthwise convolution, как в v1) и затем сжимает с помощью 1х1-свертки. При этом к последней сжимающей свертке не применяется нелинейная функция (поэтому этот слой называется linear bottleneck). Этот linear bottleneck затем подается на вход следующего модуля. Таким образом, на входе и на выходе модуля находятся linear bottlenecks. Авторы предполагают, что такое сжатие не приводит к потере информации, так как каналы избыточны и каждый "пиксель" можно закодировать, используя меньшее кол-во каналов (это аналогично тому, как точки на двухмерной плоскости в трехмерном пространстве можно описать, используя две координаты вместо трех). Нелинейность не применяется к такому сжатому тензору, так как это может разрушить слишком много информации. Так как такие сжатые тензоры содержат всю необходимую информацию, авторы применяют шорткат к боттлнекам (inverted residuals), а не к слоям с большим количеством каналов (traditional residuals). Такие шорткаты позволяют улучшить прохождение градиентного сигнала. Inverted Residuals и Linear Bottlenecks позволяют уменьшить кол-во параметров и операций, а также увеличить быстродействие по сравнению с v1, при этом не уменьшая точность.

Mobilenet_v3 - Searching for MobileNetV3 [https://arxiv.org/pdf/1905.02244.pdf]
Авторы представляют две модели для разных сценариев использования в зависимости от доступных вычислительных ресурсов (large and small). При создании моделей сначала использовались алгоритмы автоматического поиска оптимальной архитектуры (алгоритмы MnasNet для оптимизации блоков и NetAdapt для поиска оптимального количества фильтров для каждого слоя). Затем в полученных моделях они уменьшают в два раза число фильтров в первом слое, а также передвигают самый тяжелый сверточный слой в конце сети на место после финального average pooling, чтобы уменьшить latency. Также в слоях из второй половины сети они используют нелинейность hard swish вместо Relu, которая улучшает точность. Кроме того, авторы используют блок squeeze-and-excite (по сути это механизм attention, позволяющий сети научиться выделять более важные каналы и ослаблять менее важные). В итоге модель состоит из практически таких же блоков, как v2 (1x1 expansion convolution, depthwise convolution, 1x1 compression convolution, shortcut). Отличия заключаются в использовании hard swish вместо relu, а также в применении squeeze-and-excite к тензору, полученному после depthwise convolution. В результате v3 точнее и быстрее, чем v2.


2. Инструкция по запуску обучения
    1) установить torch и torchvision (я также использовал numpy и Pillow, но они подтягиваются при установке torch и torchvision).
    2) ввести команду python train.py. Если CUDA доступна, то начнется обучение на GPU. Если нет - на CPU. Обучение производится на датасете CIFAR100. По умолчанию тренироваться будет модель Small с width multiplier=1.0, на протяжении 20 итераций, с batch size=128. Эти параметры можно поменять с помощью флагов. Пример: python train.py --model large --width 0.5 --iter 40 --batch 64
    3) в итоге модель к 20 итерации достигнет top-1 accuracy на CIFAR100 около 70% для маленькой модели и около 72% для большой. Лучшая модель сохраняется в папку trained_models. 

    Результаты обучения:
    Mobilenet_v3_small_0.25 val_acc: 60.13%
    Mobilenet_v3_small_0.50 val_acc: 66.37%
    Mobilenet_v3_small_1.00 val_acc: 69.44%
    Mobilenet_v3_large_0.25 val_acc: 65.03%
    Mobilenet_v3_large_0.50 val_acc: 69.50%
    Mobilenet_v3_large_1.00 val_acc: 71.75%


3. Инструкция по запуску тестового скрипта
    1) поместить тестовые изображения в папку test_images. Однако это необязательно, так как я уже поместил туда 20 изображений.
    2) ввести команду python test.py. Скрипт, для каждой обученной модели, которая есть в папке trained_models, произведет классификацию изображений. В папке trained_models уже есть одна обученная модель для тестирования - Mobilenet_v3_large_1.0_fortest.pth.
    3) также можно измерить скорость работы, запустив скрипт latency.py. Скрипт измерит скорость моделей с разными значениями width multiplier на GPU и CPU.


4. Реализованные и возможные улучшения
    Реализованные:
    1) в первых трех слоях, у которых stride равен 2, я понизил stride до 1, так как изображения в CIFAR100 имеют низкое разрешение 32х32. Если этого не сделать, то финальные слои, у которых фильтры имеют размер 5х5, будут применяться к тензорам с разрешением 1х1, что не имеет смысла. Top-1 accuracy при этом будет около 30%. Если же поменять stride, то финальные слои будут применяться к тензорам 8х8, что близко к разрешению 7х7 в статье. Top-1 accuracy при этом повысится до 70%.
    2) изначально я пробовал использовать параметры для тренировки, описанные в пункте 6.1.1 статьи. Однако обучение было медленным и точность низкой. Я поменял оптимизатор на SGD with momentum, а также не использую L2-regularization и dropout, так как они не влияли на точность в моих экспериментах. Кроме того, я добавил аугментацию изображений (RandomCrop и RandomHorizontalFlip), что позволило повысить точность на 2-3%.

    Возможные:
    1) можно реализовать функцию hard swish в виде кусочно-линейной функции, как сделали авторы статьи, что позволит значительно увеличить скорость.
    2) можно уменьшить количество каналов в последних слоях нейросети без потери точности, так как последние слои оптимизированы для классификации на ImageNet (1000 классов), в то время как в CIFAR100 всего 100 классов.
    3) я использую дефолтную инициализацию всех параметров. Помимо этого я пробовал только torch.nn.init.kaiming_normal_, но разницы в точности не было в моих экспериментах. Однако возможно имеет смысл попробовать другие схемы инициализации. 
    4) возможно добавление других способов аугментации помимо RandomCrop и RandomHorizontalFlip позволит повысить точность.


5. Проблемы
    1) в статье в таблице для модели small указано, что в последнем блоке перед Pool 7x7 применяется SE-block, что довольно странно, так как во всех остальных случаях этот блок применяется только внутри боттлнеков. Я посмотрел официальный код [https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v3.py], и в нем нет SE-блока в этом месте, поэтому я его тоже не использую.
